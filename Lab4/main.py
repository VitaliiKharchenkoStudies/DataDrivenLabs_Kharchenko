# -*- coding: utf-8 -*-
"""4_lab_PINNs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w76YqFr6TWVbQ4rh8Ks-rnhZqbvdJcH_

### Physics-informed neural networks (PINNS) ###
A Physics-Informed Neural Network (PINN) is a type of neural network architecture designed to incorporate physical principles or equations into the learning process. It combines deep learning techniques with domain-specific knowledge, making it particularly suitable for problems governed by physics. [1]

PINNs are designed to solve supervised learning tasks for modeling physical systems and phenomena. This could involve tasks like modeling fluid flows, quantum mechanics, combustion, etc. [1]
"""

# Import all necessary packages
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

"""Here we will build a basic neural network architecture using PyTorch. The provided code defines a function called buildModel that helps us create a neural network with sequential layers.

*Input Layer:* The **torch.nn.Linear**(inDim, hidDim) is a linear layer used in neural networks that applies a linear transformation to input data using weights and biases. It is a critical component in the architecture of many deep learning models.

*Hidden Layer:* The **torch.nn.Tanh()** activation function which outputs values between -1 and 1, with a mean output of 0. This can help ensure that the output of a neural network layer remains centered around 0, making it useful for normalization purposes. Tanh is a smooth and continuous activation function, which makes it easier to optimize during the process of gradient descent. [2]

The tanh function can be susceptible to the vanishing gradient problem, especially for deep neural networks with many layers. This is because the slope of the function becomes very small for large or small input values, making it difficult for gradients to propagate through the network.

Also, due to the use of exponential functions, tanh can be computationally expensive, especially for large tensors or when used in deep neural networks with many layers. [2]

*Output Layer:* The final layer is another linear transformation, torch.nn.Linear(hidDim, outDim), which maps the hidden layer's outputs to the desired output dimension, outDim.
"""


def buildModel(inDim, hidDim, outDim):
    model = nn.Sequential(
        nn.Linear(inDim, hidDim),  # first layer
        nn.Tanh(),  # activation function
        nn.Linear(hidDim, outDim)  # output layer
    )
    return model


"""This code snippet introduces three essential functions for PINNs:

**Gradient Calculation (get_derivative):** We will use PyTorch's automatic differentiation to calculate the gradients. The get_derivative function determines the gradients of a tensor 'y' with respect to another tensor 'x'.

**Equation Residual (get_residual):** The get_residual function represents the difference between the left and right sides of our differential equation and will be used to calculate the loss.

**Boundary Condition Residual (get_boundary_residual):** The get_boundary_residual function will be used to enforce the boundary conditions of our problem through predicting the values and comparing them with the actual values at the boundaries. This is crucial for ensuring the neural network's predictions adhere to physical constraints.
"""


def get_derivative(y, x):
    return torch.autograd.grad(y, x, torch.ones(x.size()[0], 1), create_graph=True, retain_graph=True)[0]


# Additional functions for PINNs:
def get_residual(model, x):
    u = model(x)
    u_x = get_derivative(u, x)
    u_xx = get_derivative(u_x, x)
    residual = u_xx + u  # Example differential equation: u'' + u = 0
    return residual


def get_boundary_residual(model, x_boundary, u_boundary):
    u_pred = model(x_boundary)
    boundary_residual = u_pred - u_boundary
    return boundary_residual


# Example usage:
# Define model dimensions
inDim = 1
hidDim = 10
outDim = 1

# Build model
model = buildModel(inDim, hidDim, outDim)

# Define optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop (simplified)
for epoch in range(1000):
    x_interior = torch.rand((10, 1), requires_grad=True)  # Random interior points
    x_boundary = torch.tensor([[0.0], [1.0]], requires_grad=True)  # Boundary points
    u_boundary = torch.tensor([[0.0], [0.0]])  # Boundary values

    optimizer.zero_grad()

    # Compute residuals
    residual_interior = get_residual(model, x_interior)
    residual_boundary = get_boundary_residual(model, x_boundary, u_boundary)

    # Compute loss
    loss_interior = torch.mean(residual_interior ** 2)
    loss_boundary = torch.mean(residual_boundary ** 2)
    loss = loss_interior + loss_boundary

    # Backpropagation
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')

# Visualize results (optional)
x_test = torch.linspace(0, 1, 100).view(-1, 1)
u_test = model(x_test).detach().numpy()
plt.plot(x_test.numpy(), u_test)
plt.xlabel('x')
plt.ylabel('u')
plt.title('PINN Solution')
plt.show()
